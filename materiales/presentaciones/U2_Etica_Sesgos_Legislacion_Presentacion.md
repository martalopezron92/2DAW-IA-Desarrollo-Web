# ğŸ¤” Ã‰tica, Sesgos y LegislaciÃ³n en IA
## PresentaciÃ³n interactiva para 2Âº DAW

---

## ğŸš¨ Pregunta de apertura

### Â¿Puede una mÃ¡quina ser racista?

<!-- **ğŸ’­ Reflexiona:**
- Â¿Has notado alguna vez que un algoritmo te trata de forma diferente?
- Â¿Crees que los algoritmos son mÃ¡s objetivos que los humanos?
- Â¿QuÃ© pasarÃ­a si una IA decidiera si consigues un trabajo?

**â° Tiempo de reflexiÃ³n: 2 minutos** -->

---

## ğŸ¯ Lo que vamos a descubrir

### Al final de esta sesiÃ³n entenderÃ¡s:
- âœ… **Por quÃ©** la IA puede ser discriminatoria
- âœ… **CÃ³mo** detectar y prevenir sesgos algorÃ­tmicos
- âœ… **QuÃ©** dice la ley sobre IA en Europa
- âœ… **CÃ³mo** desarrollar IA de forma responsable

---

## ğŸ¤– Caso real: Amazon y la IA machista

### ğŸ“° **Los hechos (2018):**
Amazon desarrollÃ³ un sistema de IA para evaluar currÃ­culums automÃ¡ticamente.

### ğŸš¨ **El problema:**
```python
# Lo que hacÃ­a el algoritmo (simplificado)
def evaluar_curriculum(cv):
    puntuacion = 0
    
 
    if "women's" in cv:  # ej: "women's chess club captain"
        puntuacion -= 10
    
    if "female" in cv:   # ej: "female software engineer"
        puntuacion -= 5
    
    if "executed" in cv or "captured" in cv:
        puntuacion += 10
    
    return puntuacion
```

### ğŸ¤” **Pregunta clave:**
**Â¿Por quÃ© crees que la IA se volviÃ³ "machista"?**

---

## ğŸ§  La respuesta: Datos sesgados

### ğŸ“Š **El origen del problema:**

| Datos de entrenamiento | Resultado |
|------------------------|-----------|
| ğŸ“ˆ **10 aÃ±os de CVs exitosos** | â†’ Mayoritariamente hombres |
| ğŸ¢ **Sector tecnolÃ³gico** | â†’ HistÃ³ricamente masculino |
| âœ… **CVs de contratados** | â†’ Patrones "masculinos" |

### ğŸ’¡ **La IA aprendiÃ³:**
> **"Para ser exitoso, hay que parecerse a los exitosos del pasado"**

### ğŸ”„ **El ciclo del sesgo:**
```
Sociedad sesgada â†’ Datos sesgados â†’ IA sesgada â†’ Decisiones sesgadas â†’ Sociedad mÃ¡s sesgada
```

### ğŸ’­ **Pregunta de reflexiÃ³n:**
**Â¿Es culpa de la IA o de nosotros como sociedad?**

---

## ğŸ­ Los 4 tipos de sesgos que debes conocer

### 1. ğŸ“š **Sesgo de datos histÃ³ricos**
**Â¿QuÃ© es?** Los datos reflejan discriminaciones pasadas

**Ejemplo prÃ¡ctico:**
```python
# Algoritmo de crÃ©dito bancario
def aprobar_credito(codigo_postal, ingresos):
    # Si vives en zona "problemÃ¡tica" â†’ mÃ¡s difÃ­cil
    if codigo_postal in zonas_historicamente_pobres:
        umbral_ingresos = ingresos * 1.5  # MÃ¡s exigente
    else:
        umbral_ingresos = ingresos
    
    return umbral_ingresos > 30000
```

### 2. ğŸ¨ **Sesgo de representaciÃ³n**
**Â¿QuÃ© es?** Ciertos grupos estÃ¡n sub-representados

**Ejemplo famoso:**
- ğŸ‘¨ğŸ» Reconocimiento facial: 99% precisiÃ³n en hombres blancos
- ğŸ‘©ğŸ¿ Reconocimiento facial: 65% precisiÃ³n en mujeres negras

### ğŸ’­ **Â¿Por quÃ© crees que ocurre esta diferencia?**

---

## ğŸ” Los otros dos sesgos

### 3. ğŸ§  **Sesgo de confirmaciÃ³n**
**Â¿QuÃ© es?** Buscar datos que confirmen lo que ya creemos

**Ejemplo en buscadores:**
```javascript
// BÃºsqueda sesgada
function buscar(termino) {
    if (termino === "CEO exitoso") {
        // Muestra mÃ¡s resultados de hombres blancos
        return resultados.filter(r => r.demografia === "hombre_blanco");
    }
}
```

### 4. ğŸ“ **Sesgo de mediciÃ³n**
**Â¿QuÃ© es?** Medir mal o con herramientas inadecuadas

**Ejemplo:** Evaluar inteligencia solo con tests de matemÃ¡ticas
- Â¿Descarta a personas creativas?
- Â¿Favorece ciertos tipos de educaciÃ³n?

### ğŸ¤” **Pregunta clave:**
**Â¿CÃ³mo podrÃ­amos detectar estos sesgos en nuestras aplicaciones web?**

---

## ğŸ”§ Detectando sesgos: El cÃ³digo que necesitas

### ğŸ“Š **Herramienta bÃ¡sica de detecciÃ³n:**

```python
import pandas as pd

def detectar_sesgo_genero(datos):
    """Detecta sesgo de gÃ©nero en contrataciones"""
    
    # Calcular tasas por gÃ©nero
    hombres = datos[datos['genero'] == 'M']
    mujeres = datos[datos['genero'] == 'F']
    
    tasa_hombres = hombres['contratado'].mean()
    tasa_mujeres = mujeres['contratado'].mean()
    
    # Calcular disparidad
    disparidad = tasa_hombres / tasa_mujeres
    
    print(f"Tasa contrataciÃ³n hombres: {tasa_hombres:.2%}")
    print(f"Tasa contrataciÃ³n mujeres: {tasa_mujeres:.2%}")
    print(f"Disparidad: {disparidad:.2f}")
    
    # Regla prÃ¡ctica: disparidad > 1.2 o < 0.8 = posible sesgo
    if disparidad > 1.2:
        return "âš ï¸ Posible sesgo a favor de hombres"
    elif disparidad < 0.8:
        return "âš ï¸ Posible sesgo a favor de mujeres"
    else:
        return "âœ… No se detecta sesgo significativo"
```

### ğŸ’­ **Â¿QuÃ© otros grupos deberÃ­amos analizar ademÃ¡s del gÃ©nero?**

---

## âš–ï¸ Â¿QuÃ© dice la ley? GDPR y mÃ¡s

### ğŸ‡ªğŸ‡º **GDPR: Tus derechos ante la IA**

#### ğŸ“œ **ArtÃ­culo 22 - El mÃ¡s importante:**
> *"Derecho a no ser objeto de una decisiÃ³n basada Ãºnicamente en tratamiento automatizado"*

#### ğŸ” **Â¿QuÃ© significa en la prÃ¡ctica?**

```javascript
// ANTES: DecisiÃ³n 100% automÃ¡tica
function aprobar_prestamo(datos_usuario) {
    return algoritmo_ia.decidir(datos_usuario); // âŒ Ilegal
}

// DESPUÃ‰S: SupervisiÃ³n humana obligatoria
function aprobar_prestamo_legal(datos_usuario) {
    recomendacion_ia = algoritmo_ia.sugerir(datos_usuario);
    decision_final = humano.revisar(recomendacion_ia, datos_usuario);
    
    // Guardar justificaciÃ³n
    log.guardar({
        'recomendacion_ia': recomendacion_ia,
        'decision_humana': decision_final,
        'justificacion': humano.explicacion
    });
    
    return decision_final; // âœ… Legal
}
```

### ğŸ›¡ï¸ **Tus derechos como usuario:**
1. **ğŸ” Derecho de explicaciÃ³n:** "Â¿Por quÃ© me negaron el crÃ©dito?"
2. **ğŸ“¥ Portabilidad:** Llevarte tus datos a otra empresa
3. **ğŸ—‘ï¸ Derecho al olvido:** Eliminar tus datos
4. **ğŸ‘¥ SupervisiÃ³n humana:** Que una persona revise decisiones importantes

---

## ğŸ‡ªğŸ‡º Nueva Ley de IA Europea (2024)

### ğŸš¦ **Sistema de semÃ¡foros por riesgo:**

#### ğŸ”´ **RIESGO INACEPTABLE** (Prohibido)
- ğŸ›ï¸ Sistemas de puntuaciÃ³n social (como en China)
- ğŸ§  ManipulaciÃ³n subliminal del comportamiento
- ğŸ‘ï¸ CategorizaciÃ³n biomÃ©trica por raza/religiÃ³n

#### ğŸŸ  **ALTO RIESGO** (RegulaciÃ³n estricta)
- ğŸ’¼ SelecciÃ³n de personal automatizada
- ğŸ’° EvaluaciÃ³n crediticia
- ğŸ“ Sistemas de calificaciÃ³n educativa

### ğŸ’­ **Â¿En quÃ© categorÃ­a pondrÃ­as un sistema de recomendaciÃ³n de Netflix?**

---

## ğŸŸ¡ **RIESGO LIMITADO** (Transparencia obligatoria)

### ğŸ“± **Ejemplos que usas diariamente:**
- ğŸ¤– Chatbots y asistentes virtuales
- ğŸ“º Sistemas de recomendaciÃ³n (Netflix, YouTube)
- ğŸ–¼ï¸ Deepfakes y contenido generado por IA

### ğŸ“‹ **Obligaciones legales:**
```html
<!-- ANTES: Sin avisos -->
<div class="chat">
    <p>Â¡Hola! Â¿En quÃ© puedo ayudarte?</p>
</div>

<!-- DESPUÃ‰S: Transparencia obligatoria -->
<div class="chat">
    <div class="ai-notice">
        ğŸ¤– Este chat es atendido por inteligencia artificial
        <a href="/info-ai">MÃ¡s informaciÃ³n</a>
    </div>
    <p>Â¡Hola! Â¿En quÃ© puedo ayudarte?</p>
</div>
```

### ğŸ¤” **Â¿Has visto avisos asÃ­ en alguna web o app?**

---

## ğŸŸ¢ Los 5 pilares de la IA Ã©tica

### 1. âš–ï¸ **Fairness (Equidad)**
**Â¿QuÃ© significa?** Tratamiento justo para todos

**En la prÃ¡ctica:**
```python
# Verificar equidad en recomendaciones
def verificar_equidad_recomendaciones(usuarios, recomendaciones):
    # Verificar diversidad por gÃ©nero
    rec_hombres = recomendaciones[usuarios['genero'] == 'M']
    rec_mujeres = recomendaciones[usuarios['genero'] == 'F']
    
    # Â¿Los hombres y mujeres reciben diversidad similar?
    diversidad_h = calcular_diversidad(rec_hombres)
    diversidad_m = calcular_diversidad(rec_mujeres)
    
    if abs(diversidad_h - diversidad_m) > 0.1:
        return "âš ï¸ Posible inequidad en diversidad"
    return "âœ… Equidad verificada"
```

### ğŸ’­ **Â¿QuÃ© otros aspectos deberÃ­amos verificar ademÃ¡s del gÃ©nero?**

---

## ğŸ” Los otros 4 pilares Ã©ticos

### 2. ğŸ“Š **Transparency (Transparencia)**
**Â¿QuÃ© hace tu IA?** Explicarlo claramente

```javascript
// Interfaz transparente
class TransparentAI {
    mostrarExplicacion() {
        return `
            <div class="ai-explanation">
                <h3>Â¿CÃ³mo funciona nuestra IA?</h3>
                <p>ğŸ“Š Analizamos tu historial de compras</p>
                <p>ğŸ‘¥ Comparamos con usuarios similares</p>
                <p>ğŸ¯ Sugerimos productos que podrÃ­an gustarte</p>
                <button onclick="verMasDetalles()">Ver mÃ¡s detalles</button>
            </div>
        `;
    }
}
```

### 3. ğŸ¯ **Explainability (Explicabilidad)**
**Â¿Por quÃ© esta decisiÃ³n?** Poder explicar cada resultado

### 4. ğŸ›¡ï¸ **Accountability (Responsabilidad)**
**Â¿QuiÃ©n responde si algo sale mal?** Responsabilidades claras

### 5. ğŸ”’ **Privacy (Privacidad)**
**Â¿CÃ³mo proteges los datos?** Seguridad y minimizaciÃ³n

---

## ğŸ”§ IA responsable: CÃ³digo prÃ¡ctico

### ğŸ’» **Consentimiento transparente:**

```javascript
class AIConsentManager {
    constructor() {
        this.consentimientos = {};
    }
    
    solicitarConsentimiento(tipoIA) {
        const explicaciones = {
            'recomendacion': `
                <div class="consent-card">
                    <h3>ğŸ¯ Sistema de Recomendaciones</h3>
                    <p><strong>Â¿QuÃ© hace?</strong> Analiza tus compras para sugerir productos</p>
                    <p><strong>Â¿QuÃ© datos usa?</strong> Historial, navegaciÃ³n, preferencias</p>
                    <p><strong>Â¿Puedes controlarlo?</strong> SÃ­, en configuraciÃ³n de privacidad</p>
                    <p><strong>Â¿QuiÃ©n decide?</strong> El algoritmo sugiere, tÃº eliges</p>
                    
                    <button onclick="aceptar('recomendacion')">âœ… Acepto</button>
                    <button onclick="rechazar('recomendacion')">âŒ No gracias</button>
                </div>
            `
        };
        
        return explicaciones[tipoIA];
    }
}
```

### ğŸ¤” **Â¿QuÃ© informaciÃ³n adicional incluirÃ­as para ser mÃ¡s transparente?**

---

## ğŸ“Š Sistema de monitoreo Ã©tico

### ğŸ” **Dashboard de mÃ©tricas Ã©ticas:**

```python
class EthicsMonitor:
    def generar_reporte_semanal(self):
        """Reporte automÃ¡tico de mÃ©tricas Ã©ticas"""
        
        metricas = {
            # Equidad
            'disparidad_genero': self.calcular_disparidad('genero'),
            'disparidad_edad': self.calcular_disparidad('edad'),
            
            # Transparencia
            'usuarios_informados': self.porcentaje_usuarios_informados(),
            'explicaciones_solicitadas': self.count_explicaciones(),
            
            # Privacidad
            'datos_minimizados': self.verificar_minimizacion(),
            'consentimientos_validos': self.verificar_consentimientos()
        }
        
        # Generar alertas si algo va mal
        alertas = []
        if metricas['disparidad_genero'] > 1.2:
            alertas.append("ğŸš¨ Disparidad de gÃ©nero detectada")
        
        if metricas['usuarios_informados'] < 0.8:
            alertas.append("âš ï¸ Baja transparencia con usuarios")
            
        return {
            'metricas': metricas,
            'alertas': alertas,
            'recomendaciones': self.generar_recomendaciones(metricas)
        }
```

### ğŸ’­ **Â¿QuÃ© otras mÃ©tricas deberÃ­amos monitorear?**

---

## ğŸ”¥ Caso Netflix: Â¿CÃ³mo lo hacen bien?

### ğŸ¬ **El desafÃ­o:**
- ğŸ“Š **200+ millones** de usuarios
- ğŸŒ **190+ paÃ­ses** diferentes
- ğŸ­ **Diversidad** cultural enorme
- âš–ï¸ **Equilibrio** entre engagement y responsabilidad

### âœ… **Su enfoque Ã©tico:**

```python
class NetflixEthicalRecommender:
    def generar_recomendaciones(self, usuario):
        # 1. Recomendaciones base por preferencias
        base_recs = self.algoritmo_preferencias(usuario)
        
        # 2. Aplicar filtros de diversidad
        diverse_recs = self.aplicar_diversidad(base_recs, {
            'generos_minimos': 4,      # Al menos 4 gÃ©neros diferentes
            'idiomas_minimos': 2,      # Al menos 2 idiomas
            'epocas_variadas': True,   # Contenido nuevo y clÃ¡sico
            'creadores_diversos': True # Directores de diferentes backgrounds
        })
        
        # 3. Evitar cÃ¡maras de eco
        balanced_recs = self.evitar_polarizacion(diverse_recs)
        
        # 4. Respetar controles parentales y preferencias usuario
        final_recs = self.aplicar_controles_usuario(balanced_recs, usuario)
        
        return final_recs
```

### ğŸ¤” **Â¿Crees que Netflix hace suficiente para ser Ã©tico? Â¿QuÃ© mejorarÃ­as?**

---

## ğŸ’¥ Caso Microsoft Tay: Cuando todo sale mal

### ğŸ¤– **El experimento (2016):**
Microsoft lanzÃ³ un chatbot en Twitter que "aprendÃ­a" de conversaciones

### ğŸš¨ **El desastre:**
- â° **En 24 horas:** El bot se volviÃ³ racista y ofensivo
- ğŸ‘¥ **Usuarios maliciosos:** Lo "entrenaron" con contenido tÃ³xico
- ğŸ“± **Twitter:** AmplificÃ³ y viralizÃ³ el problema

### ğŸ“š **Lo que aprendimos:**

```python
# ANTES: Aprendizaje sin filtros
class TayBot:
    def aprender_de_conversacion(self, mensaje):
        # Aprende de TODO sin filtros
        self.modelo.entrenar(mensaje)  # âŒ Peligroso
        
# DESPUÃ‰S: Aprendizaje con filtros
class SafeBot:
    def aprender_de_conversacion(self, mensaje):
        # 1. Filtrar contenido tÃ³xico
        if self.detector_toxicidad(mensaje) > 0.7:
            return  # No aprender de contenido tÃ³xico
            
        # 2. Verificar fuente confiable
        if not self.fuente_confiable(usuario):
            return
            
        # 3. SupervisiÃ³n humana para casos lÃ­mite
        if self.detector_incertidumbre(mensaje) > 0.5:
            self.enviar_a_revision_humana(mensaje)
        else:
            self.modelo.entrenar(mensaje)  # âœ… Seguro
```

### ğŸ’­ **Â¿QuÃ© lecciones podemos aplicar a nuestros proyectos web?**

---

## ğŸ› ï¸ Tu checklist de IA responsable

### âœ… **Antes de lanzar cualquier IA en tu web:**

#### ğŸ“Š **Datos:**
- [ ] Â¿Los datos son representativos de todos los usuarios?
- [ ] Â¿He verificado sesgos histÃ³ricos en mis datos?
- [ ] Â¿Tengo suficiente diversidad en el dataset?

#### ğŸ¤– **Algoritmo:**
- [ ] Â¿Puedo explicar cÃ³mo funciona en tÃ©rminos simples?
- [ ] Â¿He testado el rendimiento en diferentes grupos demogrÃ¡ficos?
- [ ] Â¿Tengo mÃ©tricas de equidad ademÃ¡s de precisiÃ³n?

#### ğŸ‘¥ **Usuarios:**
- [ ] Â¿Los usuarios saben que estÃ¡n interactuando con IA?
- [ ] Â¿Pueden solicitar explicaciones de las decisiones?
- [ ] Â¿Tienen control sobre cÃ³mo se usa la IA?

#### âš–ï¸ **Legal:**
- [ ] Â¿Cumplo con GDPR y la Ley de IA europea?
- [ ] Â¿Tengo procesos de supervisiÃ³n humana?
- [ ] Â¿Puedo auditar las decisiones tomadas?

---

## ğŸ”® El futuro de la IA Ã©tica

### ğŸŒŸ **Tendencias emergentes:**

#### ğŸ” **IA Explicable (XAI)**
```python
# Futuro: IA que se explica automÃ¡ticamente
class ExplainableAI:
    def decidir_y_explicar(self, datos):
        decision = self.modelo.predict(datos)
        explicacion = self.modelo.explain(datos)
        
        return {
            'decision': decision,
            'explicacion': f"DecisiÃ³n basada en: {explicacion.factores_principales}",
            'confianza': explicacion.nivel_confianza,
            'alternativas': explicacion.escenarios_alternativos
        }
```

#### ğŸ›¡ï¸ **IA Federated Learning**
- Entrenar modelos sin centralizar datos
- Mayor privacidad por diseÃ±o
- ColaboraciÃ³n sin comprometer seguridad

### ğŸ’­ **Â¿QuÃ© otros avances crees que veremos en IA Ã©tica?**

---

## ğŸ¯ Pregunta final de reflexiÃ³n

### ğŸ¤” **Como futuro desarrollador web:**

**Â¿CuÃ¡l es tu responsabilidad Ã©tica al crear aplicaciones con IA?**

**Piensa en:**
- ğŸ‘¥ Los usuarios que las usarÃ¡n
- ğŸŒ El impacto social de tus decisiones
- âš–ï¸ La equidad y justicia en tus algoritmos
- ğŸ”® Las consecuencias a largo plazo

### ğŸ“ **Comparte tu reflexiÃ³n:**
*Â¿CÃ³mo te asegurarÃ¡s de que tu IA sea Ã©tica y responsable?*

---

## ğŸ”— PrÃ³ximos pasos

### ğŸ“š **Para profundizar:**
1. ğŸ“– Lee el **EU AI Act** completo
2. ğŸ¬ Ve el documental **"Coded Bias"**
3. ğŸ› ï¸ Experimenta con **AI Fairness 360** de IBM
4. ğŸ‘¥ Ãšnete a comunidades de **AI Ethics**

### ğŸ¯ **En tu prÃ³ximo proyecto:**
- Implementa un checklist Ã©tico
- Incluye mÃ©tricas de equidad
- DiseÃ±a interfaces transparentes
- Planifica auditorÃ­as regulares

---

## â“ Espacio para preguntas

### ğŸ—£ï¸ **Temas para debatir:**

1. **ğŸ¤– vs ğŸ‘¥** Â¿Puede la IA ser mÃ¡s justa que los humanos?

2. **ğŸ”’ vs ğŸ¯** Â¿Privacidad o personalizaciÃ³n? Â¿Hay que elegir?

3. **âš–ï¸ vs ğŸ’°** Â¿Ã‰tica o beneficios? Â¿Son compatibles?

4. **ğŸŒ vs ğŸ¢** Â¿QuiÃ©n debe regular la IA: gobiernos o empresas?

### â° **Tiempo para preguntas: 15 minutos**

---

## ğŸ‰ Mensaje final

### ğŸ’¡ **Recuerda:**

> **"Con gran poder (de IA) viene gran responsabilidad"**

**Tu cÃ³digo puede:**
- ğŸŒŸ Hacer el mundo mÃ¡s justo y equitativo
- ğŸš¨ O perpetuar y amplificar discriminaciones

### ğŸ¯ **La decisiÃ³n es tuya como desarrollador**

**Â¿QuÃ© tipo de futuro digital quieres construir?**

---

**ğŸ”— PrÃ³xima clase: Fundamentos tÃ©cnicos de Python para IA**

*Â¡Gracias por vuestra atenciÃ³n y participaciÃ³n!* ğŸ™Œ