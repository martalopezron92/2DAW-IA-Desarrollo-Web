# ğŸ¤” Ã‰tica, Sesgos y LegislaciÃ³n en IA
## PresentaciÃ³n interactiva para 2Âº DAW

---

## ğŸš¨ Pregunta de apertura

### Â¿Puede una mÃ¡quina ser racista?

<!-- **ğŸ’­ Reflexiona:**
- Â¿Has notado alguna vez que un algoritmo te trata de forma diferente?
- Â¿Crees que los algoritmos son mÃ¡s objetivos que los humanos?
- Â¿QuÃ© pasarÃ­a si una IA decidiera si consigues un trabajo?

**â° Tiempo de reflexiÃ³n: 2 minutos** -->

---

## ğŸ¯ Lo que vamos a descubrir

### Al final de esta sesiÃ³n entenderÃ¡s:
- âœ… **Por quÃ©** la IA puede ser discriminatoria
- âœ… **CÃ³mo** detectar y prevenir sesgos algorÃ­tmicos
- âœ… **QuÃ©** dice la ley sobre IA en Europa
- âœ… **CÃ³mo** desarrollar IA de forma responsable

---

## ğŸ¤– Caso real: Amazon y la IA machista

### ğŸ“° **Los hechos (2018):**
Amazon desarrollÃ³ un sistema de IA para evaluar currÃ­culums automÃ¡ticamente.

### ğŸš¨ **El problema:**
```python
# Lo que hacÃ­a el algoritmo (simplificado)
def evaluar_curriculum(cv):
    puntuacion = 0
    
 
    if "women's" in cv:  # ej: "women's chess club captain"
        puntuacion -= 10
    
    if "female" in cv:   # ej: "female software engineer"
        puntuacion -= 5
    
    if "executed" in cv or "captured" in cv:
        puntuacion += 10
    
    return puntuacion
```

### ğŸ¤” **Pregunta clave:**
**Â¿Por quÃ© crees que la IA se volviÃ³ "machista"?**

---

## ğŸ§  La respuesta: Datos sesgados

### ğŸ“Š **El origen del problema:**

| Datos de entrenamiento | Resultado |
|------------------------|-----------|
| ğŸ“ˆ **10 aÃ±os de CVs exitosos** | â†’ Mayoritariamente hombres |
| ğŸ¢ **Sector tecnolÃ³gico** | â†’ HistÃ³ricamente masculino |
| âœ… **CVs de contratados** | â†’ Patrones "masculinos" |

### ğŸ’¡ **La IA aprendiÃ³:**
> **"Para ser exitoso, hay que parecerse a los exitosos del pasado"**

### ğŸ”„ **El ciclo del sesgo:**
```
Sociedad sesgada â†’ Datos sesgados â†’ IA sesgada â†’ Decisiones sesgadas â†’ Sociedad mÃ¡s sesgada
```

### ğŸ’­ **Pregunta de reflexiÃ³n:**
**Â¿Es culpa de la IA o de nosotros como sociedad?**

---

## ğŸ­ Los 4 tipos de sesgos que debes conocer

### 1. ğŸ“š **Sesgo de datos histÃ³ricos**
**Â¿QuÃ© es?** Los datos reflejan discriminaciones pasadas

**Ejemplo prÃ¡ctico:**
```python
# Algoritmo de crÃ©dito bancario
def aprobar_credito(codigo_postal, ingresos):
    # Si vives en zona "problemÃ¡tica" â†’ mÃ¡s difÃ­cil
    if codigo_postal in zonas_historicamente_pobres:
        umbral_ingresos = ingresos * 1.5  # MÃ¡s exigente
    else:
        umbral_ingresos = ingresos
    
    return umbral_ingresos > 30000
```

### 2. ğŸ¨ **Sesgo de representaciÃ³n**
**Â¿QuÃ© es?** Ciertos grupos estÃ¡n sub-representados

**Ejemplo famoso:**
- ğŸ‘¨ğŸ» Reconocimiento facial: 99% precisiÃ³n en hombres blancos
- ğŸ‘©ğŸ¿ Reconocimiento facial: 65% precisiÃ³n en mujeres negras

### ğŸ’­ **Â¿Por quÃ© crees que ocurre esta diferencia?**

---

## ğŸ” Los otros dos sesgos

### 3. ğŸ§  **Sesgo de confirmaciÃ³n**
**Â¿QuÃ© es?** Buscar datos que confirmen lo que ya creemos

**Ejemplo en buscadores:**
```javascript
// BÃºsqueda sesgada
function buscar(termino) {
    if (termino === "CEO exitoso") {
        // Muestra mÃ¡s resultados de hombres blancos
        return resultados.filter(r => r.demografia === "hombre_blanco");
    }
}
```

### 4. ğŸ“ **Sesgo de mediciÃ³n**
**Â¿QuÃ© es?** Medir mal o con herramientas inadecuadas

**Ejemplo:** Evaluar inteligencia solo con tests de matemÃ¡ticas
- Â¿Descarta a personas creativas?
- Â¿Favorece ciertos tipos de educaciÃ³n?

### ğŸ¤” **Pregunta clave:**
**Â¿CÃ³mo podrÃ­amos detectar estos sesgos en nuestras aplicaciones web?**

---

## ğŸ”§ Detectando sesgos: El cÃ³digo que necesitas

### ğŸ“Š **Herramienta bÃ¡sica de detecciÃ³n:**

```python
import pandas as pd

def detectar_sesgo_genero(datos):
    """Detecta sesgo de gÃ©nero en contrataciones"""
    
    # Calcular tasas por gÃ©nero
    hombres = datos[datos['genero'] == 'M']
    mujeres = datos[datos['genero'] == 'F']
    
    tasa_hombres = hombres['contratado'].mean()
    tasa_mujeres = mujeres['contratado'].mean()
    
    # Calcular disparidad
    disparidad = tasa_hombres / tasa_mujeres
    
    print(f"Tasa contrataciÃ³n hombres: {tasa_hombres:.2%}")
    print(f"Tasa contrataciÃ³n mujeres: {tasa_mujeres:.2%}")
    print(f"Disparidad: {disparidad:.2f}")
    
    # Regla prÃ¡ctica: disparidad > 1.2 o < 0.8 = posible sesgo
    if disparidad > 1.2:
        return "âš ï¸ Posible sesgo a favor de hombres"
    elif disparidad < 0.8:
        return "âš ï¸ Posible sesgo a favor de mujeres"
    else:
        return "âœ… No se detecta sesgo significativo"
```

### ğŸ’­ **Â¿QuÃ© otros grupos deberÃ­amos analizar ademÃ¡s del gÃ©nero?**

---

## ğŸ“ MÃ©tricas avanzadas para detectar sesgos

### ğŸ¯ **Las 3 mÃ©tricas clave que todo developer debe conocer**

#### 1. ğŸ‘¥ **Demographic Parity (Paridad DemogrÃ¡fica)**
**Â¿QuÃ© mide?** Todos los grupos reciben el mismo % de resultados positivos

<!-- ```python
def calcular_demographic_parity(datos, grupo_protegido):
    """Verificar si todos los grupos tienen misma tasa de aprobaciÃ³n"""
    
    resultados = {}
    for grupo in datos[grupo_protegido].unique():
        subset = datos[datos[grupo_protegido] == grupo]
        tasa_positiva = subset['resultado_positivo'].mean()
        resultados[grupo] = tasa_positiva
    
    # Mostrar resultados
    print("ğŸ“Š Tasas de aprobaciÃ³n por grupo:")
    for grupo, tasa in resultados.items():
        print(f"  {grupo}: {tasa:.2%}")
    
    # Verificar paridad (diferencia < 5%)
    tasas = list(resultados.values())
    max_diff = max(tasas) - min(tasas)
    
    if max_diff > 0.05:
        return f"âš ï¸ ViolaciÃ³n de paridad: {max_diff:.1%} diferencia"
    else:
        return "âœ… Demographic Parity cumplida"

# Ejemplo: Sistema de prÃ©stamos
prestamos = pd.DataFrame({
    'etnia': ['blanco', 'negro', 'blanco', 'latino', 'negro', 'blanco'],
    'aprobado': [1, 0, 1, 1, 0, 1]
})

resultado = calcular_demographic_parity(prestamos, 'etnia')
``` -->

### ğŸ’­ **Â¿Es justo que todos los grupos tengan exactamente la misma tasa de aprobaciÃ³n?**

---

#### 2. âš–ï¸ **Equalized Odds (Igualdad de Oportunidades)**
**Â¿QuÃ© mide?** Misma sensibilidad y especificidad para todos los grupos
<!-- 
```python
def calcular_equalized_odds(y_real, y_pred, grupo_sensible):
    """Verificar justicia en predicciones correctas"""
    
    from sklearn.metrics import confusion_matrix
    
    resultados = {}
    for grupo in grupo_sensible.unique():
        # Filtrar por grupo
        mask = grupo_sensible == grupo
        y_true_grupo = y_real[mask]
        y_pred_grupo = y_pred[mask]
        
        # Matriz de confusiÃ³n
        tn, fp, fn, tp = confusion_matrix(y_true_grupo, y_pred_grupo).ravel()
        
        # Calcular mÃ©tricas
        sensibilidad = tp / (tp + fn) if (tp + fn) > 0 else 0  # TPR
        especificidad = tn / (tn + fp) if (tn + fp) > 0 else 0  # TNR
        
        resultados[grupo] = {
            'sensibilidad': sensibilidad,
            'especificidad': especificidad
        }
    
    # Mostrar resultados
    print("ğŸ¯ Equalized Odds por grupo:")
    for grupo, metricas in resultados.items():
        print(f"  {grupo}:")
        print(f"    Sensibilidad: {metricas['sensibilidad']:.2%}")
        print(f"    Especificidad: {metricas['especificidad']:.2%}")
    
    return resultados

# Verificar si hay equidad en detecciÃ³n de fraude
resultados_equidad = calcular_equalized_odds(
    datos['es_fraude_real'], 
    datos['prediccion_fraude'], 
    datos['grupo_demografico']
)
``` -->

### ğŸ” **Â¿QuÃ© significa en tÃ©rminos simples?**
- **Sensibilidad:** "De todos los casos reales positivos, Â¿cuÃ¡ntos detectamos?"
- **Especificidad:** "De todos los casos reales negativos, Â¿cuÃ¡ntos identificamos bien?"

---

#### 3. ğŸ¯ **Calibration (CalibraciÃ³n)**
**Â¿QuÃ© mide?** Las probabilidades predichas coinciden con la realidad para todos los grupos

<!-- ```python
def verificar_calibracion(y_real, y_proba, grupo_sensible, n_bins=5):
    """Verificar si las probabilidades estÃ¡n bien calibradas por grupo"""
    
    import numpy as np
    import matplotlib.pyplot as plt
    
    fig, axes = plt.subplots(1, len(grupo_sensible.unique()), figsize=(15, 4))
    
    for i, grupo in enumerate(grupo_sensible.unique()):
        # Filtrar por grupo
        mask = grupo_sensible == grupo
        y_true_grupo = y_real[mask]
        y_prob_grupo = y_proba[mask]
        
        # Crear bins de probabilidad
        bins = np.linspace(0, 1, n_bins + 1)
        bin_centers = (bins[:-1] + bins[1:]) / 2
        
        calibracion_real = []
        for j in range(n_bins):
            # Casos en este bin de probabilidad
            in_bin = (y_prob_grupo >= bins[j]) & (y_prob_grupo < bins[j+1])
            
            if np.sum(in_bin) > 0:
                # FracciÃ³n real de positivos en este bin
                frac_positivos = np.mean(y_true_grupo[in_bin])
                calibracion_real.append(frac_positivos)
            else:
                calibracion_real.append(0)
        
        # Graficar calibraciÃ³n
        axes[i].plot(bin_centers, calibracion_real, 'o-', label=f'Real {grupo}')
        axes[i].plot([0, 1], [0, 1], '--', label='CalibraciÃ³n perfecta')
        axes[i].set_title(f'CalibraciÃ³n - {grupo}')
        axes[i].set_xlabel('Probabilidad predicha')
        axes[i].set_ylabel('FracciÃ³n real de positivos')
        axes[i].legend()
    
    plt.tight_layout()
    return fig

# Ejemplo: Â¿Las probabilidades de aprobaciÃ³n estÃ¡n bien calibradas?
fig = verificar_calibracion(
    datos['aprobado_real'], 
    datos['probabilidad_aprobacion'], 
    datos['grupo_etnico']
)
``` -->

### ğŸª **AnalogÃ­a de la calibraciÃ³n:**
> **"Si digo que llueve con 80% probabilidad, debe llover 8 de cada 10 veces que hago esa predicciÃ³n"**



---

## ğŸ› ï¸ Estrategias para mitigar sesgos

### ğŸ”§ **3 niveles de intervenciÃ³n:**


**En los datos**
- RecolecciÃ³n mÃ¡s representativa
- TÃ©cnicas de balanceo (oversampling, undersampling)
- Datos sintÃ©ticos para grupos minoritarios

**En el algoritmo**
- RegularizaciÃ³n para penalizar discriminaciÃ³n
- Restricciones de equidad durante entrenamiento
- Post-procesamiento de resultados

**En la implementaciÃ³n**
- Monitoreo continuo de mÃ©tricas de equidad
- AuditorÃ­as regulares por grupos externos
- Feedback loops para detectar deriva

<!-- #### 1. ğŸ“Š **Pre-procesamiento: Arreglar los datos**

```python
# Estrategia 1: Balanceo de datos
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler

def balancear_dataset(X, y, grupo_protegido):
    """Balancear representaciÃ³n de grupos minoritarios"""
    
    # SMOTE para generar datos sintÃ©ticos
    smote = SMOTE(random_state=42)
    X_balanced, y_balanced = smote.fit_resample(X, y)
    
    print(f"ğŸ“ˆ Dataset original: {len(X)} muestras")
    print(f"ğŸ“ˆ Dataset balanceado: {len(X_balanced)} muestras")
    
    return X_balanced, y_balanced

# Estrategia 2: Re-sampling por grupos
def balanceo_por_grupos(datos, grupo_protegido, target):
    """Asegurar representaciÃ³n equitativa"""
    
    balanced_data = []
    
    # Encontrar el grupo con menos muestras positivas
    min_positivos = float('inf')
    for grupo in datos[grupo_protegido].unique():
        subset = datos[datos[grupo_protegido] == grupo]
        n_positivos = sum(subset[target] == 1)
        min_positivos = min(min_positivos, n_positivos)
    
    # Balancear cada grupo
    for grupo in datos[grupo_protegido].unique():
        subset = datos[datos[grupo_protegido] == grupo]
        
        # Tomar muestra balanceada
        positivos = subset[subset[target] == 1].sample(min_positivos)
        negativos = subset[subset[target] == 0].sample(min_positivos)
        
        balanced_data.append(pd.concat([positivos, negativos]))
    
    return pd.concat(balanced_data, ignore_index=True)
```

#### 2. âš™ï¸ **In-processing: Entrenar con restricciones de equidad**

```python
# Estrategia 3: PenalizaciÃ³n de discriminaciÃ³n
from sklearn.linear_model import LogisticRegression
import numpy as np

class FairLogisticRegression:
    def __init__(self, fairness_penalty=0.1):
        self.fairness_penalty = fairness_penalty
        self.model = LogisticRegression()
    
    def fit(self, X, y, grupo_protegido):
        """Entrenar con penalizaciÃ³n por discriminaciÃ³n"""
        
        # FunciÃ³n de pÃ©rdida personalizada
        def loss_with_fairness(predictions):
            # PÃ©rdida normal
            loss_normal = self.calcular_loss_normal(y, predictions)
            
            # PenalizaciÃ³n por inequidad
            penalty = 0
            for grupo in np.unique(grupo_protegido):
                mask = grupo_protegido == grupo
                tasa_grupo = np.mean(predictions[mask])
                tasa_global = np.mean(predictions)
                penalty += abs(tasa_grupo - tasa_global)
            
            return loss_normal + self.fairness_penalty * penalty
        
        # Entrenar con pÃ©rdida modificada
        self.model.fit(X, y)
        return self
    
    def predict(self, X):
        return self.model.predict(X)

# Uso del modelo justo
modelo_justo = FairLogisticRegression(fairness_penalty=0.2)
modelo_justo.fit(X_train, y_train, grupos_train)
```

#### 3. ğŸ”„ **Post-procesamiento: Ajustar resultados**

```python
# Estrategia 4: Threshold optimization
def optimizar_umbrales_justos(y_real, y_proba, grupo_protegido):
    """Encontrar umbrales diferentes por grupo para equidad"""
    
    from sklearn.metrics import roc_curve
    
    umbrales_optimos = {}
    
    for grupo in np.unique(grupo_protegido):
        mask = grupo_protegido == grupo
        y_true_grupo = y_real[mask]
        y_prob_grupo = y_proba[mask]
        
        # Calcular ROC
        fpr, tpr, thresholds = roc_curve(y_true_grupo, y_prob_grupo)
        
        # Encontrar umbral que maximiza TPR - FPR (Youden's index)
        j_scores = tpr - fpr
        optimal_idx = np.argmax(j_scores)
        optimal_threshold = thresholds[optimal_idx]
        
        umbrales_optimos[grupo] = optimal_threshold
        
        print(f"ğŸ¯ Umbral Ã³ptimo para {grupo}: {optimal_threshold:.3f}")
    
    return umbrales_optimos

# Estrategia 5: CalibraciÃ³n post-hoc
def calibrar_por_grupos(y_real, y_proba, grupo_protegido):
    """Calibrar probabilidades separadamente por grupo"""
    
    from sklearn.calibration import CalibratedClassifierCV
    
    modelos_calibrados = {}
    
    for grupo in np.unique(grupo_protegido):
        mask = grupo_protegido == grupo
        
        # Crear y entrenar calibrador para este grupo
        calibrador = CalibratedClassifierCV(method='platt')
        
        # Pseudo-entrenamiento (en prÃ¡ctica usarÃ­as validation set)
        X_dummy = y_proba[mask].reshape(-1, 1)
        calibrador.fit(X_dummy, y_real[mask])
        
        modelos_calibrados[grupo] = calibrador
    
    return modelos_calibrados

def aplicar_calibracion_justa(y_proba, grupo_protegido, calibradores):
    """Aplicar calibraciÃ³n especÃ­fica por grupo"""
    
    y_proba_calibrada = np.zeros_like(y_proba)
    
    for grupo in np.unique(grupo_protegido):
        mask = grupo_protegido == grupo
        X_grupo = y_proba[mask].reshape(-1, 1)
        
        y_proba_calibrada[mask] = calibradores[grupo].predict_proba(X_grupo)[:, 1]
    
    return y_proba_calibrada
```

### ğŸ’­ **Â¿CuÃ¡l de estas estrategias crees que es mÃ¡s efectiva? Â¿Por quÃ©?**

---

## ğŸ”„ Pipeline completo de IA justa

### ğŸ› ï¸ **ImplementaciÃ³n prÃ¡ctica:**

```python
class FairMLPipeline:
    def __init__(self):
        self.preprocessor = None
        self.model = None
        self.postprocessor = None
        self.fairness_metrics = {}
    
    def fit(self, X, y, grupo_protegido):
        """Pipeline completo de ML justo"""
        
        print("ğŸ”§ Iniciando pipeline de IA justa...")
        
        # 1. Pre-procesamiento
        print("ğŸ“Š Paso 1: Balanceando datos...")
        X_balanced, y_balanced = self.preprocess_for_fairness(X, y, grupo_protegido)
        
        # 2. Entrenamiento con restricciones
        print("âš™ï¸ Paso 2: Entrenando modelo con restricciones de equidad...")
        self.model = FairLogisticRegression(fairness_penalty=0.15)
        self.model.fit(X_balanced, y_balanced, grupo_protegido)
        
        # 3. EvaluaciÃ³n de equidad
        print("ğŸ“ Paso 3: Evaluando mÃ©tricas de equidad...")
        y_pred = self.model.predict(X)
        self.evaluar_equidad(y, y_pred, grupo_protegido)
        
        # 4. Post-procesamiento si es necesario
        if self.necesita_ajuste_post():
            print("ğŸ”„ Paso 4: Ajustando con post-procesamiento...")
            self.aplicar_post_procesamiento(X, y, grupo_protegido)
        
        print("âœ… Pipeline completado!")
        return self
    
    def evaluar_equidad(self, y_true, y_pred, grupo_protegido):
        """Evaluar todas las mÃ©tricas de equidad"""
        
        # Demographic Parity
        dp_score = self.calcular_demographic_parity(y_pred, grupo_protegido)
        
        # Equalized Odds  
        eo_score = self.calcular_equalized_odds(y_true, y_pred, grupo_protegido)
        
        # Overall Accuracy
        accuracy = np.mean(y_true == y_pred)
        
        self.fairness_metrics = {
            'demographic_parity': dp_score,
            'equalized_odds': eo_score,
            'accuracy': accuracy
        }
        
        # Reportar resultados
        print("ğŸ“Š MÃ©tricas de Equidad:")
        print(f"  Demographic Parity: {dp_score}")
        print(f"  Equalized Odds: {eo_score}")
        print(f"  Accuracy General: {accuracy:.2%}")
    
    def necesita_ajuste_post(self):
        """Determinar si necesitamos post-procesamiento"""
        return (self.fairness_metrics['demographic_parity'] != "âœ…" or 
                self.fairness_metrics['equalized_odds'] != "âœ…")

# Uso del pipeline
pipeline = FairMLPipeline()
pipeline.fit(X_train, y_train, grupos_train)

# Predicciones justas
y_pred_fair = pipeline.predict(X_test)
```

### ğŸ¯ **Â¿CÃ³mo implementarÃ­as esto en tu prÃ³ximo proyecto web?** -->

---

## âš–ï¸ Â¿QuÃ© dice la ley?

### ğŸ‡ªğŸ‡º **GDPR: Tus derechos ante la IA**

#### ğŸ“œ **ArtÃ­culo 22 - El mÃ¡s importante:**
> *"Derecho a no ser objeto de una decisiÃ³n basada Ãºnicamente en tratamiento automatizado"*

---

## ğŸŸ¢ Los 5 pilares de la IA Ã©tica

### 1. âš–ï¸ **Fairness (Equidad)**
**Â¿QuÃ© significa?** Tratamiento justo para todos

### 2. ğŸ“Š **Transparency (Transparencia)**
**Â¿QuÃ© hace tu IA?** Explicarlo claramente

### 3. ğŸ¯ **Explainability (Explicabilidad)**
**Â¿Por quÃ© esta decisiÃ³n?** Poder explicar cada resultado

### 4. ğŸ›¡ï¸ **Accountability (Responsabilidad)**
**Â¿QuiÃ©n responde si algo sale mal?** Responsabilidades claras

### 5. ğŸ”’ **Privacy (Privacidad)**
**Â¿CÃ³mo proteges los datos?** Seguridad y minimizaciÃ³n

---

## ğŸ‡ªğŸ‡º Nueva Ley de IA Europea (2024)

### ğŸš¦ **Sistema de semÃ¡foros por riesgo:**

#### ğŸ”´ **RIESGO INACEPTABLE** (Prohibido)
- ğŸ›ï¸ Sistemas de puntuaciÃ³n social (como en China)
- ğŸ§  ManipulaciÃ³n subliminal del comportamiento
- ğŸ‘ï¸ CategorizaciÃ³n biomÃ©trica por raza/religiÃ³n

#### ğŸŸ  **ALTO RIESGO** (RegulaciÃ³n estricta)
- ğŸ’¼ SelecciÃ³n de personal automatizada
- ğŸ’° EvaluaciÃ³n crediticia
- ğŸ“ Sistemas de calificaciÃ³n educativa

## ğŸŸ¡ **RIESGO LIMITADO** (Transparencia obligatoria)

### ğŸ“± **Ejemplos que usas diariamente:**
- ğŸ¤– Chatbots y asistentes virtuales
- ğŸ“º Sistemas de recomendaciÃ³n (Netflix, YouTube)
- ğŸ–¼ï¸ Deepfakes y contenido generado por IA

---

## ğŸ”¥ Caso Netflix: Â¿CÃ³mo lo hacen bien?

### ğŸ¬ **El desafÃ­o:**
- ğŸ“Š **200+ millones** de usuarios
- ğŸŒ **190+ paÃ­ses** diferentes
- ğŸ­ **Diversidad** cultural enorme
- âš–ï¸ **Equilibrio** entre engagement y responsabilidad

### âœ… **Su enfoque Ã©tico:**

```python
class NetflixEthicalRecommender:
    def generar_recomendaciones(self, usuario):
        # 1. Recomendaciones base por preferencias
        base_recs = self.algoritmo_preferencias(usuario)
        
        # 2. Aplicar filtros de diversidad
        diverse_recs = self.aplicar_diversidad(base_recs, {
            'generos_minimos': 4,      # Al menos 4 gÃ©neros diferentes
            'idiomas_minimos': 2,      # Al menos 2 idiomas
            'epocas_variadas': True,   # Contenido nuevo y clÃ¡sico
            'creadores_diversos': True # Directores de diferentes backgrounds
        })
        
        # 3. Evitar cÃ¡maras de eco
        balanced_recs = self.evitar_polarizacion(diverse_recs)
        
        # 4. Respetar controles parentales y preferencias usuario
        final_recs = self.aplicar_controles_usuario(balanced_recs, usuario)
        
        return final_recs
```

---

## ğŸ’¥ Caso Microsoft Tay: Cuando todo sale mal

### ğŸ¤– **El experimento (2016):**
Microsoft lanzÃ³ un chatbot en Twitter que "aprendÃ­a" de conversaciones

### ğŸš¨ **El desastre:**
- â° **En 24 horas:** El bot se volviÃ³ racista y ofensivo
- ğŸ‘¥ **Usuarios maliciosos:** Lo "entrenaron" con contenido tÃ³xico
- ğŸ“± **Twitter:** AmplificÃ³ y viralizÃ³ el problema

### ğŸ“š **Lo que aprendimos:**

```python
# ANTES: Aprendizaje sin filtros
class TayBot:
    def aprender_de_conversacion(self, mensaje):
        # Aprende de TODO sin filtros
        self.modelo.entrenar(mensaje)  # âŒ Peligroso
        
# DESPUÃ‰S: Aprendizaje con filtros
class SafeBot:
    def aprender_de_conversacion(self, mensaje):
        # 1. Filtrar contenido tÃ³xico
        if self.detector_toxicidad(mensaje) > 0.7:
            return  # No aprender de contenido tÃ³xico
            
        # 2. Verificar fuente confiable
        if not self.fuente_confiable(usuario):
            return
            
        # 3. SupervisiÃ³n humana para casos lÃ­mite
        if self.detector_incertidumbre(mensaje) > 0.5:
            self.enviar_a_revision_humana(mensaje)
        else:
            self.modelo.entrenar(mensaje)  # âœ… Seguro
```

---

## ğŸ”® El futuro de la IA Ã©tica

### ğŸŒŸ **Tendencias emergentes:**

#### ğŸ” **IA Explicable (XAI)**
- Algoritmos y tÃ©cnicas que permiten entender las decisiones tomadas por la IA
- IA Potente + Explicabilidad = AdopciÃ³n + Confianza + Legalidad

#### ğŸ›¡ï¸ **IA Federated Learning**
- Entrenar modelos sin centralizar datos
- Mayor privacidad por diseÃ±o
- ColaboraciÃ³n sin comprometer seguridad

---

## â“ Espacio para preguntas

### ğŸ—£ï¸ **Temas para debatir:**

1. **ğŸ¤– vs ğŸ‘¥** Â¿Puede la IA ser mÃ¡s justa que los humanos?

2. **ğŸ”’ vs ğŸ¯** Â¿Privacidad o personalizaciÃ³n? Â¿Hay que elegir?

3. **âš–ï¸ vs ğŸ’°** Â¿Ã‰tica o beneficios? Â¿Son compatibles?

4. **ğŸŒ vs ğŸ¢** Â¿QuiÃ©n debe regular la IA: gobiernos o empresas?


---

## ğŸ‰ Mensaje final

### ğŸ’¡ **Recuerda:**

> **"Con gran poder (de IA) viene gran responsabilidad"**

**Tu cÃ³digo puede:**
- ğŸŒŸ Hacer el mundo mÃ¡s justo y equitativo
- ğŸš¨ O perpetuar y amplificar discriminaciones

### ğŸ¯ **La decisiÃ³n es tuya como desarrollador**

**Â¿QuÃ© tipo de futuro digital quieres construir?**
**Piensa en:**
- ğŸ‘¥ Los usuarios que las usarÃ¡n
- ğŸŒ El impacto social de tus decisiones
- âš–ï¸ La equidad y justicia en tus algoritmos
- ğŸ”® Las consecuencias a largo plazo


---

**ğŸ”— PrÃ³xima clase: Fundamentos tÃ©cnicos de Python para IA**

*Â¡Gracias por vuestra atenciÃ³n y participaciÃ³n!* ğŸ™Œ