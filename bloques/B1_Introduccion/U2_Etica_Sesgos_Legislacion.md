# U2 - tica, sesgos y legislaci贸n en IA

##  Objetivos de aprendizaje

Al finalizar esta unidad, el estudiante ser谩 capaz de:

- **Analizar** las implicaciones 茅ticas del uso de IA en aplicaciones web
- **Identificar** diferentes tipos de sesgos algor铆tmicos y sus consecuencias
- **Evaluar** el marco legal actual que regula la IA (GDPR, Ley de IA de la UE)
- **Aplicar** principios de IA responsable en el desarrollo de proyectos
- **Argumentar** sobre dilemas 茅ticos en sistemas automatizados de decisi贸n

## 憋 Duraci贸n estimada

**9 horas** distribuidas en 3 semanas (3 sesiones de 3 horas cada una)

##  Contenidos

### 1. Principios 茅ticos en IA

#### Los cinco pilares de la IA 茅tica

**1. Fairness (Equidad)**
- Tratamiento justo para todos los grupos demogr谩ficos
- Ausencia de discriminaci贸n sistem谩tica
- Igualdad de oportunidades en los resultados

**2. Accountability (Responsabilidad)**
- Capacidad de explicar decisiones algor铆tmicas
- Responsabilidad clara en caso de errores
- Mecanismos de supervisi贸n y control

**3. Transparency (Transparencia)**
- Claridad sobre c贸mo funcionan los algoritmos
- Informaci贸n accesible para usuarios finales
- Documentaci贸n de procesos de entrenamiento

**4. Explainability (Explicabilidad)**
- Capacidad de interpretar resultados
- Comunicaci贸n comprensible de decisiones
- Trazabilidad del razonamiento algor铆tmico

**5. Privacy (Privacidad)**
- Protecci贸n de datos personales
- Minimizaci贸n de recolecci贸n de informaci贸n
- Seguridad en el almacenamiento y procesamiento

#### Dilemas 茅ticos frecuentes

**Caso 1: Sistema de contrataci贸n autom谩tico**
```
Problema: Un algoritmo de selecci贸n de CVs descarta 
sistem谩ticamente candidatas mujeres para puestos t茅cnicos.

An谩lisis 茅tico:
- 驴Es el algoritmo sesgado o refleja sesgos hist贸ricos?
- 驴Qui茅n es responsable: desarrolladores, empresa, datos?
- 驴C贸mo balancear eficiencia vs equidad?
```

**Caso 2: Recomendador de contenido**
```
Problema: Una plataforma de videos recomienda contenido 
cada vez m谩s extremo para mantener engagement.

Consideraciones:
- Responsabilidad social vs beneficios comerciales
- Libertad de elecci贸n vs manipulaci贸n algor铆tmica
- Impacto en la sociedad y polarizaci贸n
```

### 2. Sesgos algor铆tmicos

#### Tipos de sesgos

**Sesgo de datos hist贸ricos**
- Los datos reflejan discriminaciones pasadas
- Perpetuaci贸n de desigualdades existentes
- Ejemplo: Algoritmos de cr茅dito que discriminan por c贸digo postal

**Sesgo de representaci贸n**
- Ciertos grupos est谩n sub-representados en los datos
- Menor precisi贸n para minor铆as
- Ejemplo: Reconocimiento facial menos preciso en personas de piel oscura

**Sesgo de confirmaci贸n**
- Selecci贸n de datos que confirman hip贸tesis previas
- Interpretaci贸n sesgada de resultados
- Ejemplo: B煤squedas que refuerzan estereotipos

**Sesgo de medici贸n**
- Diferentes formas de medir pueden favorece a ciertos grupos
- M茅tricas inadecuadas para el contexto
- Ejemplo: Evaluar inteligencia solo con un tipo de test

#### Detecci贸n de sesgos

**An谩lisis estad铆stico**
```python
# Ejemplo: Detectar sesgo por g茅nero en contrataci贸n
import pandas as pd

# Cargar datos de contrataci贸n
contrataciones = pd.read_csv('contrataciones.csv')

# Calcular tasas por g茅nero
tasa_hombres = contrataciones[contrataciones['genero'] == 'M']['contratado'].mean()
tasa_mujeres = contrataciones[contrataciones['genero'] == 'F']['contratado'].mean()

# Calcular disparidad
disparidad = tasa_hombres / tasa_mujeres
print(f"Disparidad de g茅nero: {disparidad:.2f}")

# Si disparidad > 1.2 o < 0.8, posible sesgo
if disparidad > 1.2 or disparidad < 0.8:
    print("锔 Posible sesgo detectado")
```

**M茅tricas de equidad**
- **Demographic Parity**: Misma tasa de predicci贸n positiva por grupo
- **Equalized Odds**: Misma sensibilidad y especificidad por grupo
- **Calibration**: Misma precisi贸n de predicciones por grupo

#### Mitigaci贸n de sesgos

**En los datos**
- Recolecci贸n m谩s representativa
- T茅cnicas de balanceo (oversampling, undersampling)
- Datos sint茅ticos para grupos minoritarios

**En el algoritmo**
- Regularizaci贸n para penalizar discriminaci贸n
- Restricciones de equidad durante entrenamiento
- Post-procesamiento de resultados

**En la implementaci贸n**
- Monitoreo continuo de m茅tricas de equidad
- Auditor铆as regulares por grupos externos
- Feedback loops para detectar deriva

### 3. Marco legal y regulatorio

#### GDPR (Reglamento General de Protecci贸n de Datos)

**Principios clave para IA**
- **Consentimiento expl铆cito**: Los usuarios deben saber c贸mo se usan sus datos
- **Derecho de explicaci贸n**: Los usuarios pueden solicitar explicaciones de decisiones automatizadas
- **Portabilidad de datos**: Los usuarios pueden transferir sus datos
- **Derecho al olvido**: Los usuarios pueden solicitar eliminaci贸n de datos

**Art铆culo 22: Decisiones automatizadas**
```
"El interesado tendr谩 derecho a no ser objeto de una decisi贸n 
basada 煤nicamente en el tratamiento automatizado, incluida la 
elaboraci贸n de perfiles, que produzca efectos jur铆dicos en 茅l 
o le afecte significativamente de modo similar."
```

**Implicaciones para desarrollo web**
- Interfaces claras para gesti贸n de consentimiento
- Sistemas de explicaci贸n de algoritmos
- Herramientas de descarga/eliminaci贸n de datos

#### Ley de IA de la Uni贸n Europea (2024)

**Clasificaci贸n por riesgo**

**Riesgo inaceptable (Prohibidas)**
- Sistemas de puntuaci贸n social gubernamental
- Manipulaci贸n subliminal de comportamiento
- Categorizaci贸n biom茅trica por caracter铆sticas sensibles

**Alto riesgo (Regulaci贸n estricta)**
- Sistemas de gesti贸n de recursos humanos
- Evaluaci贸n crediticia y seguros
- Sistemas educativos de evaluaci贸n

**Riesgo limitado (Transparencia obligatoria)**
- Chatbots y asistentes virtuales
- Sistemas de recomendaci贸n
- Deepfakes y contenido generado por IA

**Riesgo m铆nimo (Sin restricciones especiales)**
- Filtros de spam
- Videojuegos con IA
- Sistemas de inventario

#### Otras normativas relevantes

**Algorithmic Accountability Act (EE.UU.)**
- Auditor铆as obligatorias para sistemas de alto impacto
- Evaluaci贸n de sesgos y discriminaci贸n
- Transparencia en procesos algor铆tmicos

**Principios de IA de la OCDE**
- IA centrada en el ser humano
- Transparencia y explicabilidad
- Robustez, seguridad y protecci贸n
- Responsabilidad y rendici贸n de cuentas

### 4. IA responsable en la pr谩ctica

#### Desarrollo de aplicaciones web 茅ticas

**Fase de dise帽o**
```python
# Checklist de consideraciones 茅ticas
class EthicalChecklist:
    def __init__(self, proyecto):
        self.proyecto = proyecto
        self.checks = {
            'datos_representativos': False,
            'metricas_equidad': False,
            'explicabilidad': False,
            'consentimiento_claro': False,
            'auditoria_sesgos': False
        }
    
    def evaluar_datos(self):
        """Verificar representatividad de datos"""
        # An谩lisis demogr谩fico del dataset
        # Verificar balance entre grupos
        # Documentar limitaciones
        pass
    
    def definir_metricas(self):
        """Establecer m茅tricas de equidad"""
        # Adem谩s de precisi贸n, incluir:
        # - Paridad demogr谩fica
        # - Igualdad de oportunidades
        # - Calibraci贸n por grupos
        pass
```

**Fase de implementaci贸n**
```javascript
// Ejemplo: Consentimiento transparente para IA
class AIConsentManager {
    constructor() {
        this.consentimientos = {};
    }
    
    mostrarInformacionIA(tipoIA) {
        const info = {
            'recomendacion': 'Usamos IA para personalizar tu experiencia',
            'chatbot': 'Este chat es atendido por IA, no por humanos',
            'analisis': 'Analizamos tu comportamiento para mejorar el servicio'
        };
        
        return `
            <div class="ai-consent">
                <h3>Uso de Inteligencia Artificial</h3>
                <p>${info[tipoIA]}</p>
                <details>
                    <summary>驴C贸mo funciona?</summary>
                    <p>Explicaci贸n t茅cnica accesible...</p>
                </details>
                <button onclick="this.otorgarConsentimiento('${tipoIA}')">
                    Acepto
                </button>
            </div>
        `;
    }
}
```

#### Auditor铆a y monitoreo

**Dashboard de m茅tricas 茅ticas**
```python
# Sistema de monitoreo de equidad
import matplotlib.pyplot as plt
import seaborn as sns

class EthicsMonitor:
    def __init__(self, modelo, datos):
        self.modelo = modelo
        self.datos = datos
    
    def calcular_metricas_equidad(self, grupo_protegido):
        """Calcular m茅tricas de equidad por grupo"""
        grupos = self.datos.groupby(grupo_protegido)
        
        metricas = {}
        for nombre, grupo in grupos:
            predicciones = self.modelo.predict(grupo)
            metricas[nombre] = {
                'precision': precision_score(grupo['target'], predicciones),
                'recall': recall_score(grupo['target'], predicciones),
                'f1': f1_score(grupo['target'], predicciones)
            }
        
        return metricas
    
    def generar_reporte_equidad(self):
        """Generar reporte visual de equidad"""
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        
        # Gr谩fico de precisi贸n por grupo
        # Gr谩fico de recall por grupo  
        # Gr谩fico de F1 por grupo
        
        plt.tight_layout()
        return fig
```

##  Casos de estudio reales

### Caso 1: Amazon - Algoritmo de contrataci贸n sesgado (2018)

**Contexto**: Amazon desarroll贸 un sistema de IA para evaluar curr铆culums autom谩ticamente.

**Problema identificado**:
- El sistema penalizaba CVs que inclu铆an palabras como "women's" (ej: "women's chess club captain")
- Favorec铆a sistem谩ticamente candidatos masculinos
- Se entren贸 con datos hist贸ricos de contrataciones (mayoritariamente hombres)

**Consecuencias**:
- Amazon cancel贸 el proyecto
- P茅rdida de confianza en sistemas automatizados de RRHH
- Mayor scrutinio regulatorio

**Lecciones aprendidas**:
1. Los datos hist贸ricos pueden perpetuar sesgos
2. La diversidad en equipos de desarrollo es crucial
3. Las auditor铆as externas son necesarias

### Caso 2: Netflix - Algoritmo de recomendaci贸n 茅tico

**Enfoque responsable**:
- Transparencia sobre el uso de datos
- Control de usuario sobre recomendaciones
- Diversidad en contenido recomendado

**Implementaci贸n**:
```python
# Ejemplo simplificado de recomendaci贸n diversa
class RecomendadorEtico:
    def __init__(self, usuario):
        self.usuario = usuario
        self.objetivos_diversidad = {
            'generos': 5,  # M铆nimo 5 g茅neros diferentes
            'idiomas': 2,  # Al menos 2 idiomas
            'antig眉edad': True  # Incluir contenido antiguo y nuevo
        }
    
    def generar_recomendaciones(self, num_items=10):
        recomendaciones = self.algoritmo_base()
        
        # Aplicar filtros de diversidad
        recomendaciones_diversas = self.aplicar_diversidad(recomendaciones)
        
        # Verificar que cumple objetivos 茅ticos
        if self.verificar_diversidad(recomendaciones_diversas):
            return recomendaciones_diversas
        else:
            return self.rebalancear_recomendaciones()
```

### Caso 3: Microsoft Tay - Chatbot que aprendi贸 sesgos

**Contexto**: Microsoft lanz贸 un chatbot en Twitter que aprend铆a de conversaciones p煤blicas.

**Problema**:
- En 24 horas, el bot comenz贸 a publicar contenido ofensivo y discriminatorio
- Aprendi贸 de usuarios malintencionados que lo "entrenaron" con contenido t贸xico

**Respuesta**:
- Microsoft retir贸 el bot inmediatamente
- Implement贸 filtros y moderaci贸n m谩s estricta
- Desarroll贸 principios de IA responsable

##  Ejercicios pr谩cticos

### Ejercicio 1: An谩lisis de sesgo en dataset

```python
# Dataset: contrataciones_tech.csv
# Analiza si existe sesgo de g茅nero en contrataciones

import pandas as pd
import matplotlib.pyplot as plt

# Tu c贸digo aqu铆:
# 1. Cargar el dataset
# 2. Calcular tasa de contrataci贸n por g茅nero
# 3. Visualizar las diferencias
# 4. Calcular m茅tricas de disparidad
# 5. Proponer soluciones
```

### Ejercicio 2: Consentimiento transparente

Dise帽a una interfaz web que explique claramente c贸mo una aplicaci贸n usa IA para:
- Recomendaciones de productos
- An谩lisis de sentimientos en rese帽as
- Personalizaci贸n de contenido

### Ejercicio 3: Evaluaci贸n 茅tica de proyecto

Eval煤a un proyecto de IA web usando esta r煤brica:

| Criterio | S铆 | Parcial | No |
|----------|----|---------|----|
| 驴Los datos son representativos? | | | |
| 驴Se monitorizan sesgos? | | | |
| 驴Hay transparencia sobre el uso de IA? | | | |
| 驴Los usuarios pueden controlar la IA? | | | |
| 驴Hay mecanismos de apelaci贸n? | | | |

##  Quiz de autoevaluaci贸n

### Pregunta 1
驴Cu谩l de estos NO es un principio de IA 茅tica?

**a)** Fairness (Equidad)  
**b)** Transparency (Transparencia)  
**c)** Efficiency (Eficiencia)  
**d)** Accountability (Responsabilidad)

### Pregunta 2
El sesgo de representaci贸n ocurre cuando:

**a)** Los datos reflejan discriminaciones hist贸ricas  
**b)** Ciertos grupos est谩n sub-representados en los datos de entrenamiento  
**c)** Los desarrolladores tienen prejuicios conscientes  
**d)** El algoritmo es t茅cnicamente deficiente

### Pregunta 3
Seg煤n el GDPR, los usuarios tienen derecho a:

**a)** Solo ver sus datos personales  
**b)** Obtener explicaciones de decisiones automatizadas  
**c)** Modificar algoritmos que los afecten  
**d)** Todas las anteriores

### Pregunta 4
En la Ley de IA de la UE, los chatbots se clasifican como:

**a)** Riesgo inaceptable  
**b)** Alto riesgo  
**c)** Riesgo limitado  
**d)** Riesgo m铆nimo

### Pregunta 5
La "paridad demogr谩fica" significa que:

**a)** Todos los grupos tienen el mismo tama帽o en el dataset  
**b)** El algoritmo tiene la misma precisi贸n para todos los grupos  
**c)** Todos los grupos tienen la misma tasa de predicciones positivas  
**d)** No existe discriminaci贸n en los datos

##  Proyecto: Manifiesto de IA Responsable

### Instrucciones
Crea un "Manifiesto de IA Responsable" para una empresa de desarrollo web que incluya:

1. **Principios fundamentales** (m谩ximo 5)
2. **Compromisos espec铆ficos** con ejemplos pr谩cticos
3. **Proceso de auditor铆a** para proyectos con IA
4. **M茅tricas de seguimiento** para evaluar cumplimiento
5. **Plan de respuesta** ante problemas 茅ticos

### Formato
- Documento de 2-3 p谩ginas
- Lenguaje accesible para no t茅cnicos
- Ejemplos concretos de implementaci贸n
- Referencias a legislaci贸n aplicable

##  Recursos adicionales

### Lecturas obligatorias
- **"Weapons of Math Destruction"** - Cathy O'Neil (Cap铆tulos 1-3)
- **EU AI Act** - Resumen ejecutivo oficial
- **Partnership on AI - Principios** (documento completo)

### Videos recomendados
- **"The Coded Bias"** - Documental sobre sesgos algor铆tmicos
- **"AI Ethics: The Good, The Bad, The Ugly"** - TEDx Talk
- **Cathy O'Neil en TED**: "The era of blind faith in big data must end"

### Herramientas pr谩cticas
- **AI Fairness 360** (IBM) - Detecci贸n y mitigaci贸n de sesgos
- **What-If Tool** (Google) - Exploraci贸n de modelos
- **Fairlearn** (Microsoft) - Equidad en machine learning

##  Actividades de debate

### Debate 1: "驴Deber铆a prohibirse el reconocimiento facial en espacios p煤blicos?"

**Posiciones**:
- **A favor**: Riesgos de privacidad y sesgos discriminatorios
- **En contra**: Beneficios de seguridad y conveniencia

**Preparaci贸n**: Cada estudiante debe traer 3 argumentos y 2 evidencias

### Debate 2: "IA vs Empleos: 驴Responsabilidad social o evoluci贸n natural?"

**Contexto**: Una empresa quiere automatizar atenci贸n al cliente con chatbots

**Consideraciones**:
- Impacto en empleo humano
- Beneficios para consumidores
- Responsabilidad corporativa
- Transici贸n justa para trabajadores

##  Evaluaci贸n de la unidad

Esta unidad se eval煤a mediante:
- **P02**: Debate estructurado + ensayo reflexivo (sumativa 8%)
- **Participaci贸n**: En discusiones y an谩lisis de casos
- **Proyecto**: Manifiesto de IA Responsable

---

**Unidad elaborada por**: Departamento de Inform谩tica y Comunicaciones  
**ltima actualizaci贸n**: 26 de octubre de 2025